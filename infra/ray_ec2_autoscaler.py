#!/usr/bin/env python3
"""
ec2_autoscaler.py

Generate ray_ec2_autoscaler.yaml dynamically from environment variables.
No external Python dependencies. Safe defaults included. Edit the generated
YAML to fine-tune AMI ids, KeyName, and other provider-specific values.

Usage:
    # optionally set environment variables before running, or edit defaults below
    python3 ec2_autoscaler.py

This writes ./ray_ec2_autoscaler.yaml
"""
from __future__ import annotations
import os
import sys
from pathlib import Path
import textwrap

OUT_PATH = Path("ray_ec2_autoscaler.yaml")

# ---------------------------
# Defaults (override with env vars)
# ---------------------------
REGION = os.getenv("RSV_AWS_REGION", "us-east-1")
SSH_USER = os.getenv("RSV_SSH_USER", "ubuntu")
SSH_PRIVATE_KEY = os.getenv("RSV_SSH_PRIVATE_KEY", "~/.ssh/id_rsa")
RAY_CLUSTER_NAME = os.getenv("RSV_CLUSTER_NAME", "ray-cluster")

# Head node
HEAD_INSTANCE = os.getenv("RSV_HEAD_INSTANCE", "m5.large")
HEAD_AMI = os.getenv("RSV_HEAD_AMI", "ami-HEAD-AMI-ID")  # << REPLACE BEFORE PRODUCTION
HEAD_KEYNAME = os.getenv("RSV_HEAD_KEYNAME", "your-ec2-keypair")  # << REPLACE

# CPU worker nodes
CPU_INSTANCE = os.getenv("RSV_CPU_INSTANCE", "m5.xlarge")
CPU_AMI = os.getenv("RSV_CPU_AMI", "ami-CPU-AMI-ID")  # << REPLACE
CPU_MIN = int(os.getenv("RSV_CPU_MIN_WORKERS", "1"))
CPU_MAX = int(os.getenv("RSV_CPU_MAX_WORKERS", "6"))

# GPU worker nodes
GPU_INSTANCE = os.getenv("RSV_GPU_INSTANCE", "p3.2xlarge")
GPU_AMI = os.getenv("RSV_GPU_AMI", "ami-GPU-AMI-ID")  # << REPLACE (must have CUDA & drivers)
GPU_MIN = int(os.getenv("RSV_GPU_MIN_WORKERS", "0"))
GPU_MAX = int(os.getenv("RSV_GPU_MAX_WORKERS", "4"))

# Cluster scaling limits
MIN_WORKERS = int(os.getenv("RSV_CLUSTER_MIN_WORKERS", "0"))
MAX_WORKERS = int(os.getenv("RSV_CLUSTER_MAX_WORKERS", "10"))
IDLE_TIMEOUT_MINUTES = int(os.getenv("RSV_IDLE_TIMEOUT_MINUTES", "10"))

# Node resource tokens (custom)
CPU_NODE_TOKEN = os.getenv("RSV_CPU_NODE_TOKEN", "CPU_NODE")
GPU_NODE_TOKEN = os.getenv("RSV_GPU_NODE_TOKEN", "GPU_NODE")

# Packages to install on setup (comma-separated)
DEFAULT_PIP_PACKAGES = os.getenv(
    "RSV_PIP_PACKAGES",
    "ray[default]==2.5.0 httpx transformers"
)
# on GPU nodes prefer onnxruntime-gpu
GPU_PIP_PACKAGES = os.getenv(
    "RSV_GPU_PIP_PACKAGES",
    "ray[default]==2.5.0 httpx transformers onnxruntime-gpu"
)

# Safety minor validation
required_placeholders = {
    "HEAD_AMI": HEAD_AMI,
    "CPU_AMI": CPU_AMI,
    "GPU_AMI": GPU_AMI,
    "HEAD_KEYNAME": HEAD_KEYNAME,
}
placeholders_warn = [k for k, v in required_placeholders.items() if v.upper().endswith("AMI-ID") or v.startswith("your-")]
if placeholders_warn:
    print("WARNING: Some AMI / KeyName defaults look like placeholders. Replace them before production:")
    for p in placeholders_warn:
        print("  -", p)
    print()

# ---------------------------
# YAML template generation
# ---------------------------
yaml_text = f"""\
# ray_ec2_autoscaler.yaml - generated by ec2_autoscaler.py
# Edit AMI ids, KeyName, region before calling `ray up`.

cluster_name: {RAY_CLUSTER_NAME}
min_workers: {MIN_WORKERS}
max_workers: {MAX_WORKERS}

provider:
  type: aws
  region: {REGION}

auth:
  ssh_user: {SSH_USER}
  ssh_private_key: {SSH_PRIVATE_KEY}

head_node:
  InstanceType: {HEAD_INSTANCE}
  ImageId: {HEAD_AMI}
  KeyName: {HEAD_KEYNAME}

available_node_types:
  ray.head.default:
    node_config:
      InstanceType: {HEAD_INSTANCE}
      ImageId: {HEAD_AMI}
    max_workers: 0
    resources: {{"CPU": 4}}

  ray.worker.cpu:
    node_config:
      InstanceType: {CPU_INSTANCE}
      ImageId: {CPU_AMI}
    min_workers: {CPU_MIN}
    max_workers: {CPU_MAX}
    resources: {{"{CPU_NODE_TOKEN}": 1, "CPU": 8}}
    setup_commands:
      - "sudo apt-get update -y"
      - "sudo apt-get install -y python3 python3-pip git build-essential"
      - "python3 -m pip install -U pip"
      - "pip3 install {DEFAULT_PIP_PACKAGES}"
      - "if [ ! -d /opt/myrepo ]; then git clone https://github.com/you/your-repo.git /opt/myrepo; fi"
      - "cd /opt/myrepo || true"

  ray.worker.gpu:
    node_config:
      InstanceType: {GPU_INSTANCE}
      ImageId: {GPU_AMI}
    min_workers: {GPU_MIN}
    max_workers: {GPU_MAX}
    resources: {{"{GPU_NODE_TOKEN}": 1, "CPU": 8, "GPU": 1}}
    setup_commands:
      - "sudo apt-get update -y"
      - "sudo apt-get install -y python3 python3-pip git build-essential"
      - "python3 -m pip install -U pip"
      - "pip3 install {GPU_PIP_PACKAGES}"
      - "if [ ! -d /opt/myrepo ]; then git clone https://github.com/you/your-repo.git /opt/myrepo; fi"
      - "cd /opt/myrepo || true"
      # Ensure the AMI has CUDA drivers and nvidia-smi

head_node_type: ray.head.default
worker_default_node_type: ray.worker.cpu

idle_timeout_minutes: {IDLE_TIMEOUT_MINUTES}

# docker: {{}}   # Configure if you run inside Docker images
"""

# Write to file atomically
try:
    OUT_PATH.write_text(textwrap.dedent(yaml_text))
    print(f"Wrote {OUT_PATH.resolve()}")
    print("IMPORTANT: Edit the YAML and replace placeholder AMI ids and KeyName before using in production.")
except Exception as e:
    print("Failed to write YAML:", e)
    sys.exit(1)
